{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch神经网络包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# 加载和显示图像\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torchvision.transforms as transforms  # 处理PIL图像并转换成Torch张量\n",
    "import torchvision.models as models  # 训练或加载预先训练的模型\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4、显示图像\n",
    "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)  # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内容损失计算\n",
    "class ContentLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target, ):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "# 风格损失计算\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载神经网络\n",
    "\n",
    "\n",
    "# 输入图片归一化\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        print(self.mean.shape)\n",
    "        print(img.shape)\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "\n",
    "\n",
    "# 定义卷积层\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img, content_img,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
    "    # to put in modules that are supposed to be activated sequentially\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            # The in-place version doesn't play very nicely with the ContentLoss\n",
    "            # and StyleLoss we insert below. So we replace with out-of-place\n",
    "            # ones here.\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # now we trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "imsize = 512 if torch.cuda.is_available() else 256\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "style_img = image_loader(\"./img/style_img.png\")\n",
    "content_img = image_loader(\"./img/content_img.png\")\n",
    "assert style_img.size() == content_img.size(), \"we need to import style and content images of the same size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "# 在每个步骤中，纠正图像以将其值保持在0-1之间。\n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps=90,\n",
    "                       style_weight=1000000, content_weight=1):\n",
    "    \n",
    "   \n",
    "    # 创建风格转换模型\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "                                                                     normalization_mean, normalization_std, style_img,\n",
    "                                                                     content_img)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 30 == 0:\n",
    "                print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "    # a last correction...\n",
    "    input_img.data.clamp_(0, 1)\n",
    "    return input_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 1])\n",
      "torch.Size([1, 4, 522, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Project\\Deeplearning\\Experiment\\Exper4\\test.ipynb 单元格 7\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cnn_normalization_mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m cnn_normalization_std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                             content_img, style_img, content_img)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m imshow(output, title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNeural Style\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32md:\\Project\\Deeplearning\\Experiment\\Exper4\\test.ipynb 单元格 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_style_transfer\u001b[39m(cnn, normalization_mean, normalization_std,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                        content_img, style_img, input_img, num_steps\u001b[39m=\u001b[39m\u001b[39m90\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                        style_weight\u001b[39m=\u001b[39m\u001b[39m1000000\u001b[39m, content_weight\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m    \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# 创建风格转换模型\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     model, style_losses, content_losses \u001b[39m=\u001b[39m get_style_model_and_losses(cnn,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                                                                      normalization_mean, normalization_std, style_img,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                                                                      content_img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     optimizer \u001b[39m=\u001b[39m get_input_optimizer(input_img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     run \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32md:\\Project\\Deeplearning\\Experiment\\Exper4\\test.ipynb 单元格 7\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     content_losses\u001b[39m.\u001b[39mappend(content_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m style_layers:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39m# add style loss:\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     target_feature \u001b[39m=\u001b[39m model(style_img)\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     style_loss \u001b[39m=\u001b[39m StyleLoss(target_feature)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     model\u001b[39m.\u001b[39madd_module(\u001b[39m\"\u001b[39m\u001b[39mstyle_loss_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i), style_loss)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\.conda\\envs\\deeplearn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\.conda\\envs\\deeplearn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Henry\\.conda\\envs\\deeplearn\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Henry\\.conda\\envs\\deeplearn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\.conda\\envs\\deeplearn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32md:\\Project\\Deeplearning\\Experiment\\Exper4\\test.ipynb 单元格 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(img\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/Deeplearning/Experiment/Exper4/test.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m (img \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstd\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# 运行算法\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                            content_img, style_img, content_img)\n",
    "plt.figure()\n",
    "imshow(output, title='Neural Style')\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
